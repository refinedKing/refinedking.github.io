<!DOCTYPE html>
<!-- saved from url=(0052)file:///C:/users/admini~1/appdata/local/temp/19.html -->
<html><head>
<link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
<link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
<link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
<link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
<link rel="stylesheet" type="text/css" href="stylesheets/makedown.css" media="print">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style>
body {
  width: 60em;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 30px;
}

</style>
<title>spark</title></head>
<header>
  <iframe src="header.html" height="100%" width="100%" frameBorder=0 marginwidth=0 marginheight=0 scrolling="no" ALLOWTRANSPARENCY="true"></iframe>
</header>
<body>
<article class="markdown-body"><h1 id="_1"><a name="user-content-_1" href="file:///C:/users/admini~1/appdata/local/temp/19.html#_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>简介:</h1>
<pre><code>spark是一个内存计算框架 , 解决了mapreduce在迭代与交互方面的不足
迭代: 多轮算法计算形式,pagerank,逻辑回归等
交互: 实时数据,数据仓库查询等
</code></pre>
<h1 id="spark"><a name="user-content-spark" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark重要概念:</h1>
<pre><code>RDD: 
    弹性分布式数据集, 弹性(数据多节点分片)分布式(集群节点)数据集(默认加载至内存)
操作:
    转换(transformation) rdd-&gt;newrdd
    # 注意此类操作不会提交作业
    map(func) 形成新的RDD集
    filter(func) 过滤结果成为新的RDD集
    flatmap(func) 映射为多于0的输出元素
    groupbykey(key) 以Key为组的数据集
    reducebykey(func) 以Key为组的数据集在经过func的处理
    join(newrdd) 连接两个数据集 k,v k,w =&gt; k,(v,w)
    sortbykey(desc) 排序数据集
动作(action) 结果处理
    # 注意此类操作将提交作业
    reduce(func) func函数输入两个返回一个结果 
    collect 返回操作后的数据元素
    count 元素个数统计
    countByKey 按Key统计每组的数目
    saveastextfile 以文件形式保存计算结果集
    saveassequencefile 以序列文件保存计算结果集
操作分类:
value:
    1-&gt;1:
        map, flatmap, mappartitions, glom
    n-&gt;1:
        union类型一致合并, cartesian分区笛卡尔积
    n-&gt;n:
        groupby
    n属于m:
        filter, distinct, subtract, sample, takesample
    cache:
        cache仅内存, persist可决定缓存位置
key-value:
    1-&gt;1:
        mapValues
    union:
        combineByKey, reduceByKey, partitionBy分区策略, cogroup协同划分
    join:
        join(cogroup,flatMapValues), leftOutJoin, rightOutJoin
action:
    无输出:
        foreach
    HDFS:
        saveAsTextFile, saveAsObjectFile
    Scala:
        collect, collectAsMap, reduceByKeyLocally, count, lookup, top(top,take,takeOrdered,first), reduce, fold, aggregate
    DataType:
</code></pre>
<h1 id="spark_1"><a name="user-content-spark_1" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark编程概念:</h1>
<pre><code>共享变量: 
    广播变量(boarkcast不可修改) .value
    累加器(accumulator可累加)  .value
初始化: 
    构建SparkContext(集群地址,作业名称,sparkhome,作业依赖项)
RDD类型: 
    并行集合与hadoop数据集
        并行集合: scala集合,可分片slice(parallelize)
        hadoop集合: 文本,序列(K,V),inputformat
        (之后均可做map/reduce操作)
Cache: 
    RDD转换后可缓存persist,cache,可容错
    cache级别: 仅内存,内存磁盘,内存序列化kryo,内存磁盘序列化,仅磁盘,
</code></pre>
<h1 id="spark_2"><a name="user-content-spark_2" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark_2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark基本架构:</h1>
<pre><code>应用程序间环境隔离 , 资源管理(YARN , Mesos) + 应用程序管理 . 程序运行过程中对RDD分片的优化(窄fusion , 宽全局barrier)
spark运行流程:
  提交应用 -&gt; DAG依赖流程化 -&gt; 执行DAG至完成 -&gt; 销毁运行环境
提交应用:
  构建运行环境: 粗(一次YARN,Mesos).细(多次Mesos)
  启动RDD管理器 BlockManager
DAG依赖流程化:
  分解宽窄依赖: 归组
    fusion优化,不需要物化
执行DAG至完成:
  数据本地化: 基架感知
  推测执行: 另起资源比较
销毁运行环境:
    回收资源
</code></pre>
<h1 id="spark-on-yarn"><a name="user-content-spark-on-yarn" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark-on-yarn" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark on YARN:</h1>
<pre><code>客户端 提交任务 至 ApplicationMaster 申请资源 向 Nodemanager
SAM 内部有 ClusterScheduler(cpu核数) 有了资源 启动 StandaloneExecutorBackend(container大小) 注册 akka 等待任务 之后执行 完毕回收资源
</code></pre>
<h1 id="mesos"><a name="user-content-mesos" href="file:///C:/users/admini~1/appdata/local/temp/19.html#mesos" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Mesos介绍:</h1>
<pre><code>集群资源管理器 twitter,douban
客户端scheduler -&gt; MesosSchedulerDriver -&gt; MesosMaster(zk) -&gt; MesosSlave(Executor)
master: 核心, 资源接入,分配Allocator,其他框架的管理
salve: 接收命令,报告状态(任务,资源)
framework scheduler: MesosSchedulerDriver第三方实现接入mesos, 双层调度(资源给框架,框架自定任务)  —&gt;  类似hadoop jobtrack
framework executor: MesosExecutorDriver第三方执行实现  —&gt; 类似hadoop tasktrack

第三方框架注册流程:
    第三方实现MesosSchedulerDriver,启动SchedulerProcess服务,MasterDetector对象(探测master通知schedulerprocess),sp会向master发送注册消息,sp确认,framework确认

mesos资源分配: 主资源公平调度算法,多维度资源,授权接入框架
    设计: 资源拒绝(slave), 资源过滤(salve), 资源回收(master)

mesos容错设计:
    master: zk
    salve: 心跳,快照运行目录

mesos vs YARN:
    设计目标一致:　通用集群资源管理方案
    容错一致: zk,多主
    在线升级: YARN的salve不支持
    调度模型一致: 双层
    调度算法: YARN除了DRF还支持容量与失败
    资源隔离一致: cgroups
    支持框架一致: mapreduce,strorm,spark等
    资源分配颗粒度: YARN不支持动态资源分配</code></pre>
<h1 id="_2"><a name="user-content-_2" href="file:///C:/users/admini~1/appdata/local/temp/19.html#_2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>网络库:</h1>
<pre><code>rpc            serialization
libprocess     protocal buffer
avro           avro
Thrift         idl
socket         kryo

libprocess设计: 消息(类型) -&gt; 消息队列 -&gt; 消息处理分发器(dispatcher) -&gt; 消息处理    高效,易用
</code></pre>
<h1 id="spark_3"><a name="user-content-spark_3" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark_3" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark工作机制:</h1>
<pre><code>应用运行提交:
spark任务调度分配:
spark io: 序列化(进程actor,NIO,netty,磁盘kyro)
                    压缩(snappy,LZF)
spark net:RPC, RMI, WEB Service, EJB
                    AKKA
spark 容错: 血统, checkpoint
</code></pre>
<h1 id="spark_4"><a name="user-content-spark_4" href="file:///C:/users/admini~1/appdata/local/temp/19.html#spark_4" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>spark编程:</h1>
<pre><code>下载源码包 , 安装maven , ./make-distribution.sh –skip-java-test构建本地模式
安装sbt打包工具
编写代码:
    python: 
        from pyspark import SparkContext
    scala: 
        import org.apache.spark.SparkContext
        import org.apache.spark.SparkContext._
        import org.apache.spark.SparkConf
编写sbt依赖文件:
    name := “Simple Project”
    version := “1.0”
    scalaVersion := “2.10”
    libraryDependencies += “org.apache.spark” %% “spark-core” % “1.1.0”
sbt打包: sbt package
提交应用: /bin/spark-submit –class “SimpleApp” –master local test/target/scala-2.10/simple-project_2.10-1.0.jar</code></pre></article></body></html>